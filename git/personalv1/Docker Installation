
/var is a standard subdirectory of the root directory in Linux and other Unix-like operating systems that contains files to which the system writes data during the course of its operation.

Among the various subdirectories within /var are /var/cache (contains cached data from application programs), /var/games (contains variable data relating to games in /usr), /var/lib (contains dynamic data libraries and files), /var/lock (contains lock files created by programs to indicate that they are using a particular file or device), /var/log (contains log files), /var/run (contains PIDs and other system information that is valid until the system is booted again) and /var/spool (contains mail, news and printer queues).

_D fuse operation permited : means docker is runing withought admin right, pass --privileged ffor that

docker run -d -it --privileged ContainerId /usr/sbin/init

/usr/sbin/init : start systemctl

netstat -tulpn | grep :22 : to look for open ports

Steps to install Docker:

1. Download pakages for docker, apt-transport https(need to enable repository to connect with https), ca-certificates
2. get key fingerprint through curl from docker site and add it to apt repository
3. Add docker repository and whatever version you needed in apt repository
4. than install docker0
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
    
f: false tls start
s: silent mode
S : show error
L: location follows redirects
    
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

sudo apt-key fingerprint 0EBFCD88


sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
   
sudo apt-get install docker-ce

Try to access folder of windows from guest OS
What is deafult gateway:
The "default gateway" is simply the IP of the next hop for traffic that is not explicitly routed anywhere else. This will, almost certainly, be an IP address on an interface of a local router. However, it may not be an address on which you can manage the router.

If this is for a "home-type network", the inside IP (your default gateway) and the management address will probably be one and the same, but in a "large/corporate-type network" I wouldn't be so sure.

netstat -rn In computing, netstat (network statistics) is a command-line network utility tool that displays network connections for the Transmission Control Protocol (both incoming and outgoing), routing tables, and a number of network interface (network interface controller or software-defined network interface) and network ...


ps - list processes
-e - show all processes, not just those belonging to the user
-f - show processes in full format (more detailed than default)
command 1 | command 2 - pass output of command 1 as input to command 2
grep find lines containing a pattern
processname - the pattern for grep to search for in the output of ps -ef



How you solved mirror error for alpine : RUN echo " efbj" >> /etc/apk/repository : means alpine some mirror are not supported by apk repo, so we need to externally add mirror

Components of Docker Daemon:

runc :
    create containers
 
containerd :
    executiung container
    push and pull images
Shim :
    stdin and stdout for containers
    update exit status to daemon 
    
However, in projects like Kubernetes, it was beneficial for containerd to be able to do additional things like push and pull images. For these reasons, containerd now does a lot more than simple container lifecycle management. However, all the extra functionality is modular and optional, meaning you can pick and choose which bits you want.

When you type commands like this into the Docker CLI, the Docker client converts them into the appropriate API payload and POSTs them to the correct API endpoint. The API is implemented in the daemon. It is the same rich, versioned, REST API that has become a hallmark of Docker, and is accepted in the industry as the de facto container API. Once the daemon receives the command to create a new container, it makes a call to containerd.

Despite its name, containerd cannot actually create containers. It uses runc to do that. It converts the required Docker image into an OCI bundle and tells runc to use this to create a new container. runc interfaces with the OS kernel to pull together all of the constructs necessary to create a container (namespaces, cgroups etc.). The container process is started as a child-process of runc, and as soon as it is started runc will exit.

Docker CLI --> command executed --> POST to reset api endpoint --> daemon recieves command to create containre--> containerd called-->converts image into an OCI bundle --> call runc --> runc will get all the resources (kernel, namespace, cgroups) --> container started as a child process of runc--> runc exit

to perform maintenance and upgrades on the Docker daemon without impacting running containers!

We mentioned earlier that containerd uses runc to create new containers. In fact, it forks a new instance of runc for every container it creates. However, once each container is created, its parent runc process exits. This means we can run hundreds of containers without having to run hundreds of runc instances. Once a container’s parent runc process exits, the associated containerd-shim process becomes the container’s parent. Some of the responsibilities the shim performs as a container’s parent include: Keeping any STDIN and STDOUT streams open so that when the daemon is restarted, the container doesn’t terminate due to pipes being closed etc. Reports the container’s exit status back to the daemon.

some of the major functionality that still exists in the daemon includes; image management, image builds, the REST API, authentication, security, core networking, and orchestration.

you can stop a container and create a new image from it. With this in mind, images are considered build-time constructs, whereas containers are run-time constructs.

The local image repository on a Linux-based Docker host is usually located at /var/lib/docker/<storage-driver>.On Windows-based Docker hosts this is C:\ ProgramData\docker\windowsfilter.

Image registries contain multiple image repositories. In turn, image repositories can contain multiple images. That might be a bit confusing, so Figure 6.2 shows a picture of an image registry

Registory-->multiple image repos-->multiple images

Docker Hub also has the concept of official repositories and unofficial repositories.

Addressing images from official repositories is as simple as giving the repository name and tag separated by a colon (:).

the latest tag doesn’t have any magical powers! Just because an image is tagged as latest does not guarantee it is the most recent image in a repository! For example, the most recent image in the alpine repository is usually tagged as edge. Moral of the story — take care when using the latest tag!

Pulling images from an unofficial repository is essentially the same — you just need to prepend the repository name with a Docker Hub username or organization name.

If you want to pull images from 3rd party registries (not Docker Hub), you need to prepend the repository name with the DNS name of the registry.

tags are arbitrary alpha-numeric values that are stored as metadata alongside the image.

If the repository you are pulling from contains images for multiple architectures and platforms, such as Linux and Windows, the command is likely to fail.

Docker provides the --filter flag to filter the list of images returned by docker image ls.

A dangling image is an image that is no longer tagged, and appears in listings as <none>:<none>. A common way they occur is when building a new image and tagging it with an existing tag. When this happens, Docker will build the new image, notice that an existing image has a matching tag, remove the tag from the existing image, give the tag to the new image. For example, you build a new image based on alpine:3.4 and tag it as dodge:challenger. Then you update the Dockerfile to replace alpine:3.4 with alpine:3.5 and run the exact same docker image build command. The build will create a new image tagged as dodge:challenger and remove the tags from the older image. The old image will become a dangling image. You can delete all dangling images on a system with the docker image prune command. If you add the -a flag, Docker will also remove all unused images (those not in use by any containers).

dangling: Accepts true or false, and returns only dangling images (true), or non-dangling images (false). 
before: Requires an image name or ID as argument, and returns all images created before it. 
since: Same as above, but returns images created after the specified image.
label: Filters images based on the presence of a label or label and value. The docker image ls command does not display labels in its output.

For all other filtering you can use reference.

You can also use the --format flag to format output using Go templates.

You can pattern match against strings in the “NAME” field, and filter output based on any of the returned columns.

By default, Docker will only display 25 lines of results. However, you can use the --limit flag to increase that to a maximum of 100.

Another way to see the layers of an image is to inspect the image with the docker image inspect command.

The docker history command shows the build history of an image and is not a strict list of layers in the image. For example, some Dockerfile instructions used to build an image do not result in layers being created. These include; “ENV”, “EXPOSE”, “CMD”, and “ENTRYPOINT”. Instead of these creating new layers, they add metadata to the image.

It’s important to understand that as additional layers are added, the image is always the combination of all layers. Take a simple example of two layers as shown in Figure 6.6. Each layer has 3 files, but the overall image has 6 files as it is the combination of both layers.

In the slightly more complex example of the three-layered image in Figure 6.7, the overall image only presents 6 files in the unified view. This is because file 7 in the top layer is an updated version of file 5 directly below (inline). In this situation, the file in the higher layer obscures the file directly below it. This allows updated versions of files to be added as new layers to the image.

Docker employs a storage driver (snapshotter in newer versions) that is responsible for stacking layers and presenting them as a single unified filesystem. Examples of storage drivers on Linux include AUFS, overlay2, devicemapper, btrfs and zfs.
 
Multiple images can, and do, share layers. This leads to efficiencies in space and performance.

Pulling images by Digest/hash:

So far, we’ve shown you how to pull images by tag, and this is by far the most common way. But it has a problem — tags are mutable! This means it’s possible to accidentally tag an image with the wrong tag. Sometimes it’s even possible to tag an image with the same tag as an existing, but different, image. This can cause problems!

Take a second to understand what just happened there… You have an image called golftrack:1.5 that has a bug. That image is being used in your production environment. You create a new version of the image that includes a fix. Then comes the mistake… you build and push the fixed image back to its repository with the same tag as the vulnerable image!. This overwrites the original image and leaves without a great way of knowing which of your production containers are running from the vulnerable image and which are running from the fixed image? Both images have the same tag!

Every time you pull an image, the docker image pull command will include the image’s digest as part of the return code. You can also view the digests of images in your Docker host’s local repository by adding the --digests flag to the docker image ls command.

Now that we know the digest of the image, we can use it when pulling the image again. This will ensure that we get exactly the image we expect!

At the time of writing, there is no native Docker command that will retrieve the digest of an image from a remote registry such as Docker Hub. This means the only way to determine the digest of an image is to pull it by tag and then make a note of its digest. This will no doubt change in the future.

The layers are where the data lives (files etc.). Each one is fully independent, and has no concept of being part of a collective image.

Each image is identified by a crypto ID that is a hash of the config object. Each layer is identified by a crypto ID that is a hash of the content it contains.

This means that changing the contents of the image, or any of its layers, will cause the associated crypto hashes to change. As a result, images and layers are immutable, and we can easily identify any changes made to either. We call these hashes content hashes.

when you push an image layer to Docker Hub, Docker Hub will attempt to verify that the image arrived without being tampered with en-route. To do this, it runs a hash against the layer and checks to see if it matches the hash that was sent. Because the layer was compressed (changed) the hash verification will fail.

To get around this, each layer also gets something called a distribution hash. This is a hash of the compressed version of the layer. When a layer is pushed and pulled from the registry, its distribution hash is included, and this is what is used to verify that the layer arrived without being tampered with.

Multi-architecture images


when new platforms and architectures, such as Windows, ARM, and s390x were added. All of a sudden we have to think about whether the image we’re pulling is built for the architecture we’re running on. This breaks the smooth experience.

Docker (image and registry specs) now supports multi-architecture images. This means a single image (repository:tag) can have an image for Linux on x64, Linux on PowerPC, Windows x64, ARM etc. Let me be clear, we’re talking about a single image tag supporting multiple platforms and architectures.

To make this happen, the Registry API supports two important constructs: 
manifest lists (new) 
manifests

The manifest list is exactly what it sounds like: a list of architectures supported by a particular image tag. Each supported architecture then has its own *manifest detailing the layers it’s composed from.

When you pull an image, your Docker client makes the relevant calls to the Docker Registry API running on Docker Hub. If a manifest list exists for the image, it will be parsed to see if an entry exists for Linux on ARM. If an ARM entry exists, the manifest for that image is retrieved and parsed for the crypto ID’s of the layers that make up the image. Each layer is then pulled from Docker Hub’s blob store.

Creating images that run on multiple architectures requires additional effort from the image publisher. Also, some software is not cross-platform. With this in mind, manifest lists are optional — if one doesn’t exist for an image, the Registry will return the normal manifest.

However, if an image layer is shared by more than one image, that layer will not be deleted until all images that reference it have been deleted.

At a high level, we can say that hypervisors perform hardware virtualization — they carve up physical hardware resources into virtual versions. On the other hand, containers perform OS virtualization — they carve up OS resources into virtual versions.

The only thing that has an impact on container start time is the time it takes to start the application it’s running.

Checking the Docker daemon:
//Run this command on Linux systems not using Systemd
$ service docker status
docker start/running, process 29393

//Run this command on Linux systems that are using Systemd
$ systemctl is-active docker
active

Checking the Docker daemon The first thing I always do when I log on to a Docker host is check that Docker is running. $ docker version

If you’re running Linux, and your user account doesn’t have permission to access the daemon, you need to make sure it’s a member of the local docker Unix group. If it isn’t, you can add it with usermod -aG docker <user> and then you’ll have to logout and log back in to your shell for the changes to take effect.

When we hit Return, the Docker client made the appropriate API calls to the Docker daemon. The Docker daemon accepted the command and searched the Docker host’s local cache to see if it already had a copy of the requested image. In the example cited, it didn’t, so it went to Docker Hub to see if it could find it there. It could, so it pulled it locally and stored it in its local cache.

Try executing some basic commands inside of the container. You might notice that some commands do not work. This is because the images we used, like almost all container images, are highly optimized for containers.

When we started the Ubuntu container in the previous section, we told it to run the Bash shell (/bin/bash). This makes the Bash shell the one and only process running inside of the container.

This means that if you type exit, to exit the Bash shell, the container will also exit (terminate). The reason for this is that a container cannot exist without a running process — killing the Bash shell kills the container’s only process, resulting in the container also being killed. This is also true of Windows containers — killing the main process in the container will also kill the container.

Press Ctrl-PQ to exit the container without terminating it.

It is possible to delete a running container with a single command by passing the -f(forcefully) flag to docker container rm.

While this example illustrates the persistent nature of containers, I should point out that volumes are the preferred way to store persistent data in containers. But at this stage of our journey I think this is an effective example of the persistent nature of containers.

And even then, if you’re storing container data in a volume, that data’s going to persist even after the container has gone.

In our previous example tshe container was running the /bin/bash app. When you kill a running container with docker container rm <container> -f, the container will be killed without warning. The procedure is quite violent — a bit like sneaking up behind the container and shooting it in the back of the head. You’re literally giving the container, and the app it’s running, no chance to straighten its affairs before being killed. However, the docker container stop command is far more polite (like pointing a gun to the containers head and saying “you’ve got 10 seconds to say any final words”). It gives the process inside of the container a heads-up that it’s about to be stopped, giving it a chance to get things in order before the end comes. Once the docker stop command returns, you can then delete the container with docker container rm. The magic behind the scenes here can be explained with Linux/POSIX signals. docker container stop sends a SIGTERM signal to the PID 1 process inside of the container. As we just said, this gives the process a chance to clean things up and gracefully shut itself down. If it doesn’t exit within 10 seconds, it will receive a SIGKILL. This is effectively the bullet to the head. But hey, it got 10 seconds to sort itself out first! docker container rm <container> -f doesn’t bother asking nicely with a SIGTERM, it goes straight to the SIGKILL. Like we said a second ago, this is like creeping up from behind and smashing it over the head. I’m not a violent person by the way!

It’s often a good idea to run containers with a restart policy. It’s a form of self-healing that enables Docker to automatically restart them after certain events or failures have occurred.

writing, the following restart policies exist: 
always 
unless-stopped 
on-failed

always :
Typing exit from the shell will kill the container’s PID 1 process and therefore kill the container. However, Docker will automatically restart it because it was started with the --restart always policy. If you issue a docker container ls command, you will see that the container’s uptime will be less than the time since it was created.even restart stopped container if docker restarts

unless -stopped:
The main difference between the always and unless-stopped policies is that containers with the --restart unless-stopped policy will not be restarted when the daemon restarts if they were in the Stopped (Exited) state. That might be a confusing sentence, so let’s walk through an example.

on-failed:
The on-failure policy will restart a container if it exits with a non-zero exit code. It will also restart containers when the Docker daemon restarts, even containers that were in the stopped state.

We know docker container run starts a new container. But this time we give it the -d flag instead of -it. -d stands for daemon mode, and tells the container to run in the background. After that, we name the container and then give it -p 80:8080. The -p flag maps ports on the Docker host to ports inside the container. This time we’re mapping port 80 on the Docker host to port 8080 inside the container.

When building a Docker image, it’s possible to embed an instruction that lists the default app you want containers using the image to run. If we run a docker image inspect against the image we used to run our container, we’ll be able to see the app that the container will run when it starts.

It’s common to build images with default commands like this, as it makes starting containers easier. It also forces a default behavior and is a form of self documentation for the image — i.e. we can inspect the image and know what app it’s supposed to run.

The directory containing the application is referred to as the build context. It’s a common practice to keep your Dockerfile in the root directory of the build context. It’s also important that Dockerfile starts with a capital “D” and is all one word. “dockerfile” and “Docker file” are not valid.

All Dockerfiles start with the FROM instruction. This will be the base layer of the image, and the rest of the app will be added on top as additional layers.

Next, the Dockerfile creates a LABEL that specifies “nigelpoulton@hotmail.com” as the maintainer of the image. Labels are simple key-value pairs and are an excellent way of adding custom metadata to an image. It’s considered a best practice to list a maintainer of an image so that other potential users have a point of contact when working with it.

The COPY . /src instruction copies in the app files from the build context. It copies these files into the image as a new layer

Next, the Dockerfile uses the WORKDIR instruction to set the working directory for the rest of the instructions in the file. This directory is relative to the image, and the info is added as metadata to the image config and not as a new layer.

Then the RUN npm install instruction uses npm to install application dependencies listed in the package.json file in the build context. It runs within the context of the WORKDIR set in the previous instruction, and installs the dependencies as a new layer in the image.

The application exposes a web service on TCP port 8080, so the Dockerfile documents this with the EXPOSE 8080 instruction. This is added as image metadata and not an image layer.

You can use the docker image inspect web:latest command to verify the configuration of the image. It will list all of the settings that were configured from the Dockerfile.

In order to push an image to Docker Hub, you need to login with your Docker ID. You also need tag the image appropriately.

Before you can push an image, you need to tag it in a special way. This is because Docker needs all of the following information when pushing an image:
Registry 
Repository 
Tag

Docker is opinionated, so you don’t need to specify values for Registry and Tag. If you don’t specify values, Docker will assume Registry=docker.io and Tag=latest. However, Docker does not have a default value for the Repository value, it gets this from the “REPOSITORY” value of the image it is pushing.

The previous docker image ls output shows our image with web as the repository name. This means a docker image push will try and push the image to docker.io/web:latest. However, I don’t have access to the web repository, all of my images have to sit within the nigelpoulton second-level namespace. This means we need to re-tag the image to include my Docker ID.

The format of the command is docker image tag <current-tag> <new-tag> and it adds an additional tag, it does not overwrite the original.

docker container run -d --name c1 \
  -p 80:8080 \
  web:latest
  
If the test does not work, try the following: Make sure that the container is up and running with the docker container ls command. The container name is c1 and you should see the port mapping as 0.0.0.0:80->8080/tcp. Check that the firewall and other network security settings are not blocking traffic to port 80 on the Docker host.


Comment lines in a Dockerfile start with the # character. All non-comment lines are Instructions. Instructions take the format INSTRUCTION argument. Instruction names are not case sensitive, but it is normal practice to write them in UPPERCASE. This makes reading the Dockerfile easier.

Examples of instructions that create new layers are FROM, RUN, and COPY. Examples of instructions that create metadata include EXPOSE, WORKDIR, ENV, and ENTRYPOINT. The basic premise is this - if an instruction is adding content such as files and programs to the image, it will create a new layer. If it is adding instructions on how to build the image and run the application, it will create metadata. You can view the instructions that were used to build the image with the docker image history command.

First. Each line corresponds to an instruction in the Dockerfile (starting from the bottom and working up). The CREATED BY column even lists the exact Dockerfile instruction that was executed.

Second. Only 4 of the lines displayed in the output create new layers (the ones with non-zero values in the SIZE column). These correspond to the FROM, RUN, and COPY instructions in the Dockerfile. Although the other instructions might look like they create layers, they actually create metadata instead of layers. The reason that the docker image history output makes it looks like all instructions create layers is an artefact of the way Docker builds and image layering used to work.

Use the docker image inspect command to confirm that only 4 layers were created.

It is considered a good practice to use images from official repositories with the FROM instruction. This is because they tend to follow best practices and be relatively free from known vulnerabilities. It is also a good idea to start from (FROM) small images as this reduces potential vulnerabilities.

You can view the output of the docker image build command to see the general process for building an image. As the following snippet shows, the basic process is: spin up a temporary container > run the Dockerfile instruction inside of that container > save the results as a new image layer > remove the temporary container.

Big means slow. Big means hard to work with. And big means a more potential vulnerabilities and possibly a bigger attack surface! For these reasons, Docker images should be small. The aim of the game is to only ship production images containing the stuff needed to run your app in production.

For example, the way you write your Dockerfiles has a huge impact on the size of your images. A common example is that every RUN instruction adds a new layer. As a result, it’s usually considered a best practice to include multiple commands as part of a single RUN instruction - all glued together with double-ampersands (&&) and backslash (\) line-breaks. While this isn’t rocket science, it requires time and discipline.

Another issue is that we don’t clean up after ourselves. We’ll RUN a command against an image that pulls some build-time tools, and we’ll leave all those tools in the image when we ship it to production. Not ideal! 

There were ways around this - most notably the builder pattern. But most of these required discipline and added complexity. 

The builder pattern required you to have at least two Dockerfiles - one for development and one for production. You’d write your Dockerfile.dev to start from a large base image, pull in any additional build tools required, and build your app. You’d then build an image from the Dockerfile.dev and create a container from it. You’d then use your Dockerfile.prod to build a new image from a smaller base image, and copy over just the application stuff from the container you just created from the build image. And everything needed to be glued together with a script.

This approach was doable, but at the expense of complexity. Multi-stage builds to the rescue!

With multi-stage builds, we have a single Dockerfile containing multiple FROM instructions. Each FROM instruction is a new build stage that can easily COPY artefacts from previous stages.

USe cases for multistage builds?

An important thing to note, is that COPY --from instructions are used to only copy production-related application code from the images built by the previous stages. They do not copy across build artefacts that are not needed for production. It’s also important to note that we only need a single Dockerfile, and no extra arguments are needed for the docker image build command!

Leverage the build cache

As we know, the docker image build process iterates through a Dockerfile one-line-at-a-time starting from the top. For each instruction, Docker looks to see if it already has an image layer for that instruction in its cache. If it does, this is a cache hit and it uses that layer. If it doesn’t, this is a cache miss and it builds a new layer from the instruction. Getting cache hits can hugely speed up the build process.

If it finds a layer, it skips the instruction, links to that existing layer, and continues the build with the cache in tact. If it does not find a layer, it invalidates the cache and builds the layer. This operation of invalidating the cache invalidates it for the remainder of the build. This means all subsequent Dockerfile instructions are completed in full without attempting to reference the build cache.

Firstly, as soon as any instruction results in a cache-miss (no layer was found for that instruction), the cache is no longer used for the rest of the entire build. This has an important impact on how you write your Dockerfiles. Try and build them in a way that places any instructions that are likely to change towards the end of the file. This means that a cache-miss will not occur until later stages of the build - allowing the build to benefit as much as possible from the cache.

You can force the build process to ignore the entire cache by passing the --no-cache=true flag to the docker image build command.

It is also important to understand that the COPY and ADD instructions include steps to ensure that the content being copied into the image has not changed since the last build. For example, it’s possible that the COPY . /src instruction in the Dockerfile has not changed since the previous, but… the contents of the directory being copied into the image have changed!

To protect against this, Docker performs a checksum against each file being copied, and compares that to a checksum of the same file in the cached layer. If the checksums do not match, the cache is invalidated and a new layer is built.

Squash the image

At a high level, Docker follows the normal process to build an image, but then adds an additional step that squashes everything into a single layer.

Squashing can be good in situations where images are starting to have a lot of layers and this isn’t ideal. And example might be when creating a new base image that you want to build other images from in the future - this is much better as a single-layer image.

On the negative side, squashed images do not share image layers. This can result in storage inefficiencies and larger push and pull operations.

Add the --squash flag to the docker image build command if you want to create a squashed image.

Figure 8.8 shows some of the inefficiencies that come with squashed images. Both images are exactly the same except for the fact that one is squashed and the other is not. The squashed image shares layers with other images on the host (saving disk space) but the squashed image does not. The squashed image will also need to send every byte to Docker Hub on a docker image push command, whereas the non-squashed image only needs to send unique layers.

Use no-install-recommends

If you are building Linux images, and using the apt package manager, you should use the no-install-recommends flag with the apt-get install command. This makes sure that apt only installs main dependencies (packages in the Depends field) and not recommended or suggested packages. This can greatly reduce the number of unwanted packages that are downloaded into your images.

The -t flag tags the image, and the -f flag lets you specify the name and location of the Dockerfile. With the -f flag, it is possible to use a Dockerfile with an arbitrary name and in an arbitrary location. The build context is where your application files exist, and this can be a directory on your local Docker host or a remote Git repo.

The FROM instruction in a Dockerfile specifies the base image for the new image you will build. It is usually the first instruction in a Dockerfile.

The RUN instruction in a Dockerfile allows you to run commands inside the image which create new layers. Each RUN instruction creates a single new layer.

The COPY instruction in a Dockerfile adds files into the image as a new layer. It is common to use the COPY instruction to copy your application code into an image.

The EXPOSE instruction in a Dockerfile documents the network port that the application uses.

The ENTRYPOINT instruction in a Dockerfile sets the default application to run when the image is started as a container.

Other Dockerfile instructions include LABEL, ENV, ONBUILD, HEALTHCHECK, CMD and more…

Docker Compose and Docker Stacks are very similar. In this chapter we’ll focus on Docker Compose, which deploys and manages multi-container applications on Docker nodes operating in single-engine mode. In a later chapter, we’ll focus on Docker Stacks. Stacks deploy and manage multi-container apps on Docker nodes operating in Swarm mode.

In the beginning was Fig. Fig was a powerful tool, created by a company called Orchard, and it was the best way to manage multi-container Docker apps. It was a Python tool that sat on top of Docker, and allowed you to define entire multi-container apps in a single YAML file. You could then deploy the app with the fig command-line tool. Fig could even manage the entire life-cycle of the app.

As things stand today, Compose is still an external Python binary that you have to install on a host running the Docker Engine. You define multi-container (multi-service) apps in a YAML file, pass the YAML file to the docker-compose binary, and Compose deploys it via the Docker Engine API.

The default name for the Compose YAML file is docker-compose.yml. However, you can use the -f flag to specify custom filenames.

The first thing to note is that the file has 4 top-level keys: 
version 
services
networks 
volumes

version: "3.5"
services:

web-fe:
    build: .
    command: python app.py
    ports:
      - target: 5000
        published: 5000
    networks:
      - counter-net
    volumes:
      - type: volume
        source: counter-vol
        target: /code
  redis:
    image: "redis:alpine"
    networks:
      counter-net:

networks:
  counter-net:

volumes:
  counter-vol:


The version key is mandatory, and it’s always the first line at the root of the file. This defines the version of the Compose file format (basically the API). You should normally use the latest version.

The top-level services key is where we define the different application services. The example we’re using defines two services; a web front-end called web-fe, and an in-memory database called redis. Compose will deploy each of these services as its own container.

The top-level networks key tells Docker to create new networks. By default, Compose will create bridge networks. These are single-host networks that can only connect containers on the same host. However, you can use the driver property to specify different network types.

The following code can be used in your Compose file to create a new overlay network called over-net that allows standalone containers to connect to it (attachable). networks:
  over-net:
  driver: overlay
  attachable: true

The top-level volumes key is where we tell Docker to create new volumes.

It’s important to understand that Compose will deploy each of these as a container, and it will use the name of the keys as part of the container names. In our example, we’ve defined two keys; web-fe and redis. This means Compose will deploy two containers, one will have web-fe in its name and the other will have redis.

docker-compose up is the most common way to bring up a Compose app (we’re calling a multi-container app defined in a Compose file a Compose app). It builds all required images, creates all required networks and volumes, and starts all required containers.

By default, docker-compose up expects the name of the Compose file to docker-compose.yml or docker-compose.yaml. If your Compose file has a different name, you need to specify it with the -f flag. The following example will deploy an application from a Compose file called prod-equus-bass.yml

It’s also common to use the -d flag to bring the app up in the background.

Our example brought the app up in the foreground (we didn’t use the -d flag), but we used the & to give us the terminal window back. This is not normal, but it will output logs directly in our terminal window which we’ll use later.

Notice how Compose has named the newly built image as a combination of the project name (counter-app), and the resource name as specified in the Compose file (web-fe). Compose has removed the dash (-) from the project name. All resources deployed by Compose will follow this naming convention.

The name of each is prefixed with the name of the project (name of the working directory). Also, each one has a numeric suffix that indicates the instance number — this is because Compose allows for scaling.

Line 12 shows the counter-net network being removed, and line 13 shows the docker-compose up process exiting.

It’s important to note that the counter-vol volume was not deleted. This is because volumes are intended to be long-term persistent data stores. As such, their lifecycle is entirely decoupled from the containers they serve. Running a docker volume ls will show that the volume is still present on the system. If you’d written any data to the volume it would still exist.

Also, any images that were built or pulled as part of the docker-compose up operation will still remain on the system. This means future deployments of the app will be faster.

You can delete a stopped Compose app with the docker-compose rm command. This will delete the containers and networks the app is using, but it will not delete volumes or images. Nor will it delete the application source code (app.py, Dockerfile, requirements.txt, and docker-compose.yml) in your project directory.

Use the docker-compose top command to list the processes running inside of each service (container).

Restart the app with the docker-compose restart command.

docker-compose always works from where docker-compose.yaml presents

Use the docker-compose down command to stop and delete the app with a single command.

The app is now deleted. Only its images, volumes and source code remain.

It’s also worth knowing that Compose builds networks and volumes before deploying services. This makes sense, as they are lower-level infrastructure objects that are consumed by services (containers). The following snippet shows Compose creating the network and volume as its first two tasks (even before building and pulling images).

If we take another look at the service definition for web-fe, we’ll see that it’s mounting the counter-app volume into the service’s container at /code. We can also see from the Dockerfile that /code is where the app is installed and executed from. Net result, our app code resides on a Docker volume.

Each Docker volume is exposed at a location within the Docker host’s filesystem, as well as a mount point in one or more containers. Use the following docker volume inspect command to find where the volume is exposed on the Docker host.

docker volume inspect counterapp_counter-vol | grep Mount

"Mountpoint": "/var/lib/docker/volumes/counterapp_counter-vol/_data",

Docker Swarm is two things: an enterprise-grade secure cluster of Docker hosts, and an engine for orchestrating microservices apps.

On the clustering front, it groups one or more Docker nodes and lets you manage them as a cluster. Out-of-the-box you get an encrypted distributed cluster store, encrypted networks, mutual TLS, secure cluster join tokens, and a PKI that makes managing and rotating certificates a breeze! And you can non-disruptively add and remove nodes. It’s a beautiful thing!

On the orchestration front, swarm exposes a rich API that allows you to deploy and manage complicated microservices apps with ease. You can define your apps in declarative manifest files, and deploy them with native Docker commands. You can even perform rolling updates, rollbacks, and scaling operations. Again, all with simple commands.

In the past, Docker Swarm was a separate product that you layered on top of the Docker engine. Since Docker 1.12 it’s fully integrated into the Docker engine and can be enabled with a single command.

On the clustering front, a swarm consists of one or more Docker nodes. These can be physical servers, VMs, Raspberry Pi’s, or cloud instances. The only requirement is that all nodes can communicate over reliable networks.

Nodes are configured as managers or workers. Managers look after the control plane of the cluster, meaning things like the state of the cluster and dispatching tasks to workers. Workers accept tasks from managers and execute them.

The configuration and state of the swarm is held in a distributed etcd database located on all managers. It’s kept in memory and is extremely up-to-date. But the best thing about it is the fact that it requires zero configuration — it’s installed as part of the swarm and just takes care of itself.

Something that’s game changing on the clustering front is the approach to security. TLS is so tightly integrated that it’s impossible to build a swarm without it. In today’s security conscious world, things like this deserve all the props they get! Anyway, swarm uses TLS to encrypt communications, authenticate nodes, and authorize roles. Automatic key rotation is also thrown in as the icing on the cake! And it all happens so smoothly that you wouldn’t even know it was there!


On the application orchestration front, the atomic unit of scheduling on a swarm is the service. This is a new object in the API, introduced along with swarm, and is a higher level construct that wraps some advanced features around containers.
























When a container is wrapped in a service we call it a task or a replica, and the service construct adds things like scaling, rolling updates, and simple rollbacks.

whats the error means :
Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.


Docker has solutions for container-to-container networks, as well as connecting to existing networks and VLANs. The latter is important for containerized apps that need to communicate with functions and services on external systems such as VM’s and physicals.

To create a smooth out-of-the-box experience, Docker ships with a set of native drivers that deal with the most common networking requirements.

How to get a Docker container's IP address from the host? --> docker inspect | grep ipaddress

sometimes its shows docker cant connect to daemon , while daemon is running --> so mostly your user has no access over docker daemon

How to copy docker images from one host to another without via repository? --> docker save -o and docker load -i

Networks in Docker : 

single host bridge network: dafult network when you create containers

command to know what are networks adapter availabel : docker network ls

comand to know detailed info about networks adapter : docker network inspect bridge

what is name of brdge apapdter to which docker host is connected to linux host bridge adapter : docker0

command to create a network : --> docker network create -d bridge localnet

docker0(linux host adapter)-->bridge(adapter for docker containers)

what if you want to connect Mysql running on localhost and nginx running on doker contianer --> try to user -net=host for nginx container --> so that they connect to each other with localhost ports only, and no need for port forwarding configuraations

for bridge mode --> u need to bind-address in mysql configurations, and mysql has to work or make connections on that IP adress only

difference bwteeen ENTRYPOINT and CMD : entrypoint --> default appliccations need to be run
CMD--> command that run for deafult applications running at entrypoint

How to explore docker container file systems: with the help of docker exec or open an port with ssh in container

How to pass env. varibles to cdocker container --> with the help -e flag

mount host directory in docker container --> -v host:container

Exposing a port to live contianre --> as if now not possible--> but you can try to use IPTAbles to forward port from container to host

force docker for clean build instead from cache --> --no-cache flag

differnece between ADD and COPY : add can be use for adding URL also

daemon logs for docker --> var/log/messages




























